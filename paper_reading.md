# LLM 微调
## LoRA http://arxiv.org/abs/2106.09685
矩阵低秩分解，h = W0x + ∆W x = W0x + BAx，A，B为低秩矩阵
首先，我应该回顾LoRA的基本结构。原参数矩阵W的维度是h×d，更新量ΔW由B和A两个矩阵相乘得到，其中A是r×d，B是h×r。所以ΔW的秩不超过r。这样，LoRA的参数数量就是A和B的总和，也就是r×d + h×r = r×(h + d)。而原来的ΔW如果是全秩的话，参数数量是h×d。当r很小的时候，比如r=8，h和d都很大的情况下，比如h=1024，d=1024，那么LoRA的参数数量是8*(1024+1024)=16384，而全秩的话是1024*1024=1048576，差距明显。
## MaLoRA http://arxiv.org/abs/2410.22782
下投影模块中引入了一个共享的低秩子空间，从而减少了冗余和计算开销。重新分配的参数被用于增强上投影模块，扩展其秩并提升模型的理论泛化能力。